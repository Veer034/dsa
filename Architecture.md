
# Interview Questions for 10+ Years Experience - Staff/Tech Lead/Architect Role




## CI/CD & DEPLOYMENT


###  Automation & Scripting
* [x] **You mentioned automated deployment scripts - what did they include?**
  * Here's how to answer the interview question about your automated deployment scripts:

    * Script Components and Capabilities

    **1. Core Deployment Orchestration**
    - Main shell script that orchestrated the entire deployment process
    - Accepted parameters for version, pod count, traffic percentage, and target service
    - Handled authentication to both AKS cluster and GitHub Container Registry
    - Pulled specific versioned images from GitHub registry before deployment

    **2. Version Management**
    - Script could deploy specific versions by tag (e.g., v1.2.3, latest, develop)
    - Maintained version history and rollback capability
    - Tagged deployments with metadata (timestamp, deployer, git commit)

    **3. Flexible Pod Scaling**
    - Option to specify exact replica count for the deployment
    - Could launch only a subset of pods initially (e.g., 2 pods instead of full 10)
    - Useful for canary deployments or testing new versions with limited resources

    **4. Traffic Splitting Capabilities**
    - Configured service mesh or ingress rules to route percentage of traffic to new pods
    - For example: route 10% traffic to new version, 90% to stable version
    - Gradually increased traffic as new version proved stable
    - Used Kubernetes services with label selectors or Istio/Linkerd for traffic management

    **5. Deployment Strategies Supported**

    **Canary Deployment:**
    - Deploy small number of new version pods alongside existing ones
    - Divert small percentage of traffic (e.g., 10-20%) to canary pods
    - Monitor metrics, errors, and performance
    - Gradually increase traffic if healthy, rollback if issues detected

    **Blue-Green Deployment:**
    - Deploy complete new version (green) alongside current version (blue)
    - Switch traffic from blue to green once validated
    - Keep blue environment for quick rollback if needed

    **Rolling Update:**
    - Gradual replacement of old pods with new ones
    - Configurable with maxSurge and maxUnavailable parameters

    ### Script Workflow

    **Pre-deployment:**
    - Validate version exists in GitHub registry
    - Check cluster health and capacity
    - Backup current deployment configuration
    - Verify namespace and service configurations

    **Deployment Phase:**
    - Authenticate to AKS and GitHub registry
    - Apply Kubernetes manifests with templated values
    - Create/update deployment with specified replica count
    - Configure service routing for traffic splitting
    - Apply ConfigMaps and Secrets if changed

    **Post-deployment:**
    - Health check verification (readiness/liveness probes)
    - Monitor pod status until all are running
    - Validate traffic routing percentages
    - Log deployment details for audit trail
    - Send notifications (Slack/email) on success/failure

    ### Key Features

    **Parameterization:**
    - Service name selection (frontend, backend, worker services)
    - Version selection from GitHub registry tags
    - Replica count override
    - Traffic percentage specification
    - Environment selection (dev, staging, prod)

    **Safety Mechanisms:**
    - Dry-run option to preview changes without applying
    - Automatic rollback on failed health checks
    - Timeout configurations to prevent stuck deployments
    - Required approval gates for production deployments

    **Integration Points:**
    - GitHub Container Registry for image storage
    - Azure AKS for orchestration
    - kubectl for cluster interaction
    - Helm for templating (if used)
    - Monitoring tools integration for deployment validation

    ### Example Usage Scenarios

    **Canary deployment:**
    ```
    ./deploy.sh -v v2.0.0 -r 2 -t 10 -s backend-api -d canary
    ```
    - Deploys 2 pods of v2.0.0
    - Routes 10% traffic to new pods
    - Rest goes to stable version

    **Full rollout:**
    ```
    ./deploy.sh -v v2.0.0 -r 10 -t 100 -s backend-api
    ```
    - Deploys all 10 pods of new version
    - Routes 100% traffic after validation

    ### Benefits of This Approach

    - **Risk mitigation**: Test new versions with limited blast radius
    - **Zero downtime**: Always maintain available pods during deployment
    - **Quick rollback**: Easy to revert if issues arise
    - **Resource efficiency**: Don't need full duplicate environment running
    - **Flexibility**: Same script handles multiple deployment patterns
    - **Automation**: Reduced manual errors and deployment time
    - **Auditability**: All deployments logged and tracked


## SYSTEM DESIGN & ARCHITECTURE
###  Architecture Design
* [ ] Design a scalable architecture for Convonest customer engagement platform
* [ ] How would you design a real-time ad decisioning engine (from Zee5 experience)?
* [ ] Design a ticket management system with SLA tracking
* [ ] How to design a system for millions of concurrent WebSocket connections?

## BEHAVIORAL & LEADERSHIP (For Tech Lead/Architect Role)
###  Technical Leadership
* [x] **How do you evaluate build vs buy decisions?**
  * Here's a concise answer for build vs buy decisions:

    I evaluate based on **four key factors**:

    1. **Core differentiation**: Build if it's our competitive advantage, buy if it's commodity (e.g., built custom 
     deployment scripts for our specific canary needs, but bought monitoring tools)

    2. **Time and cost**: Compare development cost + ongoing maintenance vs licensing fees. If building takes more than 3-6 months, usually buy wins

    3. **Expertise and complexity**: Buy for complex problems like security/compliance where vendors have specialized knowledge. Build for simpler, specific needs we understand well

    4. **Flexibility needs**: Build when we need complete control and requirements are unique. Buy when standard features suffice and vendor can handle scaling/updates

    * **Example**: For AKS deployment, I used managed Kubernetes (buy) but built custom shell scripts (build) because 
    our traffic splitting and version management requirements were specific to our use case and faster to script than configure a heavy CI/CD platform.
---
* [x] **How do you mentor junior developers?**
  * Here's a concise answer for mentoring junior developers:

    **1. Hands-on pairing**: Work together on real tasks, not just code reviews. I explain the "why" behind decisions, not just the "how" (e.g., why we chose canary deployment over blue-green for our use case)

    **2. Progressive complexity**: Start with well-defined tasks, gradually increase ambiguity. First they deploy to dev, then staging with supervision, finally production independently

    **3. Code reviews as teaching moments**: Focus on patterns and principles, not just syntax. Explain trade-offs like "this works but won't scale" and discuss alternatives

    **4. Encourage questions and experimentation**: Create safe environments (dev/staging) where they can break things and learn. When our Redis cluster failed in staging, we debugged together to understand failover mechanics

    **5. Documentation and knowledge sharing**: Have them document what they learn, present to the team. This reinforces their learning and helps others

    * **Key principle**: Give them ownership with a safety net. Let them make decisions, be available for guidance, 
    and turn mistakes into learning opportunities rather than criticism.
---
* [x] **How do you handle technical debt?**
  * Here's a concise answer for handling technical debt:

    **1. Track and categorize**: Maintain a visible backlog of technical debt with priority levels. Label as "quick wins" (< 1 day), "important" (impacts velocity), or "critical" (security/stability risks)
    
    **2. Balance with features**: Follow the **20% rule** - dedicate ~20% of each sprint to debt reduction. Never let it accumulate indefinitely or it becomes unmanageable
    
    **3. Tie to business impact**: Prioritize debt that directly affects delivery speed, system reliability, or customer experience. Example: refactored our deployment scripts when they became error-prone and slowed releases
    
    **4. Boy Scout Rule**: Leave code better than you found it. When touching legacy code for features, improve it incrementally rather than waiting for a "big rewrite"
    
    **5. Prevent new debt**: Enforce standards through code reviews, automated linting, and clear documentation. Address shortcuts immediately or schedule cleanup before they compound

    * **Key principle**: Technical debt is like financial debt - some is acceptable for speed, but you must pay it 
    down regularly with interest (maintenance cost) in mind. Never let "we'll fix it later" actually mean never.

###    Project Management
* [x] **How do you estimate engineering resources for a project?**
  * Here's a concise answer for estimating engineering resources:
    
    **1. Break down into tasks**: Decompose the project into smallest meaningful units. For AKS deployment, I broke it into: authentication setup, script development, traffic routing logic, testing, documentation - each estimated separately
    
    **2. Three-point estimation**: For each task, estimate best case, likely case, worst case. Average them with weight on likely case. Add 20-30% buffer for unknowns and integration issues
    
    **3. Account for non-coding time**: Include code reviews (15%), meetings (10%), debugging/testing (25%), documentation (10%). A "5-day coding task" is really 7-8 days of calendar time
    
    **4. Consider team experience**: Junior developers take 2-3x longer than seniors on unfamiliar tasks. Factor in learning curve, ramp-up time, and mentoring overhead
    
    **5. Use historical data**: Reference similar past projects. Our Redis cluster setup took 3 weeks; similar database migrations typically take 2-4 weeks, so that's my baseline
    
    **6. Validate with the team**: Bottom-up estimates from engineers doing the work are more accurate than top-down. Review estimates together and adjust based on their input

    * **Key principle**: Estimation is educated guessing - be transparent about uncertainty, track actuals vs 
    estimates to improve over time, and always buffer for the unexpected.
---
* [x] **Describe your SDLC experience**
  * Here's a concise answer for SDLC experience:
    
    **1. Planning & Requirements**: Collaborate with product/stakeholders to define requirements, break down features into user stories, estimate effort, and prioritize backlog
    
    **2. Design & Architecture**: Create technical design docs, review architecture decisions with team, choose appropriate technologies (e.g., Redis clustering approach, AKS deployment strategy)
    
    **3. Development**: Write code following standards, use feature branches with Git, conduct peer code reviews, write unit/integration tests alongside features
    
    **4. Testing**: Implement automated testing (unit, integration, E2E), manual QA in staging, load testing for performance validation before production
    
    **5. Deployment**: Use CI/CD pipelines for automated builds and deployments. Implemented canary deployments with gradual traffic shifting to minimize risk, monitor during rollout
    
    **6. Monitoring & Maintenance**: Set up alerts, dashboards, log aggregation. On-call rotation for production issues, post-mortems for incidents, regular tech debt cleanup
    
    **Methodology experience**: Primarily Agile/Scrum with 2-week sprints, daily standups, sprint planning/retros. Also worked with Kanban for ops-heavy teams with continuous flow

    * **Key tools**: Git/GitHub, Jenkins/GitHub Actions for CI/CD, Kubernetes for orchestration, monitoring tools for 
    observability, Jira for project tracking
---
* [x] **How do you prioritize features vs technical improvements?**
  * Here's a concise answer for prioritizing features vs technical improvements:

    **1. Assess business impact**: Features that directly drive revenue or user growth typically win, but technical improvements that unblock future velocity or prevent outages are equally critical
    
    **2. Use the 70-20-10 rule**: Roughly 70% features/customer-facing work, 20% technical improvements (refactoring, tech debt, tooling), 10% innovation/exploration. Adjust based on system health
    
    **3. Quantify technical costs**: Translate tech improvements to business terms - "refactoring deployment scripts saves 2 hours per release" or "fixing Redis cluster instability prevents $50K/hour downtime"
    
    **4. Critical path analysis**: If technical improvements enable or accelerate multiple features, they jump in priority. Example: building deployment automation enabled faster feature iterations
    
    **5. Risk-based urgency**: Security vulnerabilities, scalability bottlenecks, or stability issues that threaten production take precedence over new features
    
    **6. Negotiate with stakeholders**: Show trade-offs transparently - "we can build feature X now but slower releases, or spend 1 sprint on CI/CD improvements for 30% faster delivery going forward"
    
    **Key principle**: Don't treat them as competing - technical improvements ARE features for engineering velocity and system reliability. Balance short-term wins with long-term sustainability.
---
* [x] **How do you handle conflicts in technical decisions within the team?**
  * Here's a concise answer for handling technical conflicts:
    
    **1. Focus on data, not opinions**: Encourage POC/prototypes or benchmark tests. Example: when debating deployment strategies, we tested both canary and blue-green in staging with actual metrics before deciding
    
    **2. Define decision criteria upfront**: Agree on what matters - performance, maintainability, cost, time-to-market. Evaluate options objectively against these criteria rather than personal preferences
    
    **3. Listen to understand context**: Each person's perspective often stems from different experiences. A backend engineer worried about scalability vs frontend engineer prioritizing speed both have valid concerns
    
    **4. Seek third perspective or escalate**: If stuck, involve architect/tech lead or run a structured RFC (Request for Comments) process where team discusses and votes
    
    **5. Document and commit**: Once decided, document the rationale, get team buy-in, and move forward united. Revisit later if assumptions change, but avoid endless rehashing
    
    **6. Disagree and commit principle**: Even if someone disagrees, once the decision is made, everyone executes fully. No "I told you so" culture - retrospect objectively if issues arise
    
    * **Key principle**: Technical conflicts are healthy when handled constructively. The goal is the best solution 
    for the project, not winning an argument. Stay respectful and data-driven.
###    Architecture Decisions
* [x] **Describe a complex technical challenge you faced and how you solved it**
  1. Which place to use realtime API , and which task can be used kafka. As all analytics left on kafka and cron jobs
  2. Whole keycloak registration and auth flow with proper email validation and user verification
  3. Chat request by agent made on contact service publish kafka to angent-finder, and then agent finder finds best agents and publish on kafka topic. Which consumes by agent service and send to all connected web socket agent UI.
  4. Database how much max connection in each service, as we are using B2s have 60-70 threads. How to write code  using Mono.zip or use flatmate to handle concurrency vs blocking threads pool.
  5. Have to give proper DB pool configs considering how many pods will be running. As if pods are more same thread count will increase so calculation have to be done considering pods count.
     10 (pool - max-size) Ã— 2 pods = 20 possible connections , So DB Threads should be matched with pods threads,
     SHOW VARIABLES LIKE 'max_connections';  Current B2s has 341 (max_connections)
  6. Database indexing to make search faster but donâ€™t create to many index. In analytics when searching for email campaign we need to keep campaign fields like scheduleAt, campaignId and other fields in indexing as we will need to search faster. IN this have to make sure only index fields which are important for improving performance. But not too much indexing will make DB heavy.
     CRITICAL :  indexing on table only work till we have column name used in index from left to right.
     Example :  INDEX idx_example (A, B, C, D)
  7. INDEXING .keyword elasticsearch Putting check on which elasticseach fields to index which are important.
  8. For Security of services, put check on ingress to handle max request per Ip Address with limit/second setup.


    Query	Uses Index Efficiently?	Why
    WHERE A = 1	âœ…	Starts from leftmost column
    WHERE A = 1 AND B = 2	âœ…	Uses prefix (A,B)
    WHERE A = 1 AND B = 2 AND C = 3	âœ…	Uses prefix (A,B,C)
    WHERE B = 2	âŒ	Missing A (leftmost)
    WHERE C = 3	âŒ	Missing A, B
    WHERE A = 1 AND C = 3	âœ… (partially)	Can use index for A = 1, but C filtering happens later (not indexed)


  9. Handle the BOT attack on email open. Clicked and unsubscribed endpoint as these are open
  10. Store the secrets in azure vault, to encrypt secret password and tokens
  11. Choosing System.currentTimeMillis() vs  Instant.now(), in data analytics pipeline. crystal oscillator used by VM sometime give wrong times, to handle this VM interacts with remote server to get correct time and adjust it. None resolve the issue. Only thing is System.currentTimeMillis() is faster, and Instant.now() give too many methods. SOLUTION : Atomic clock VM used in mission critical system like stock, defence, Or Use GPS-synchronized clocks (microsecond accuracy, $5K-$50K per server) or PTP (Precision Time Protocol) hardware - syncs every microsecond, not every few minutes like NTP
  12. In Case of Ticket counter increased, put a lock on table row level for tenant specific to have proper unique ticket counter.

  * **AI-Service**: 
    * Model Training:
      1. Data generation for IT, Hospitality, Travel, insurance, healthcare, etc industry from ChatGPT, Claude, Perplexity for query, complaint, suggestion, spam
      2. Selection of best model comparing the DeBerta, Bert, RoBerta, llama.
      3. Training models on data and refactor the arguments like learning rate, data repetition count, batch sizing. And comparing performance. Storing the chunks of learning into local for use in production. DeBerta came as winner.
      4. Write code to load these learned DeBerta model and use in Mail AI, for classification of email into query, complain, suggestions, or spam
      5. Looking way to run these models Mistral in VM , used llama for tuning. But then selecting which model of mistral to choose as we have many mistral version. mistral:7b-instruct-v0.3-q3_K_M Selected
    
    * Data Forge:
      1. Chunking of data of documents, and storing section and chunk along with converting chunks into embedded vector in ES
      2. Trying to add metadata like language detection, dividing document into sections , keeping same documents data together by  divider lines.
      3. Creating threads to handle the parallel processing of incoming Docs and FAQs on kafka topic.

    * Data GenAI:
      1. Selection of AI model for data extraction and response generation. Looking into elasticsearch using embedded query, and get next and previous section chunks.
      2. Selection of model for response generation.
      3. Publishing model performance on kafka topics for analytics.
         
      4. Model	Best For	Pros	Cons
         ```
         Gemma 7B	Emails, Chatbots, Q&A	Best multilingual grammar	Newer, fewer benchmarks
         Mixtral 8x7B	Conversational chatbots	Faster & efficient	Not best for formal text
         BLOOM 176B	Emails, Formal Writing	Best non-English support	Very high VRAM needs
         LLaMA 2	General Chat & QA	Well-optimized	Not best for multilingual
         ```
         5. ðŸ–¥ï¸ Hardware Requirements to Run Multilingual Models Locally
         
         ```
         Model	VRAM (GPU) Needed	RAM Needed (CPU)	Disk Space	Speed (FP16)	Speed (Quantized)
         Gemma 7B	16GB+ GPU	32GB RAM	8GB	Fast	Faster (4-bit)
         Mixtral 8x7B	48GB+ GPU	64GB RAM	60GB	Medium	Fast (8-bit)
         BLOOM 176B	300GB+ GPU	512GB RAM	350GB	Slow	Not practical
         LLaMA 2-13B	24GB+ GPU	64GB RAM	24GB	Medium	Fast (4-bit)
         ```

    * Data MailAI:
      1. Reading emails from kafka topic, getting language , if English use DeBerta model. If any other user MistralAI for classification.
      2. Make it multi threaded to handle process and publish result on kafka topics.

  * **Redis:**
    1. Issue in setting up cluster in 3 VMs, networking issues, proper configuration. Why Lua was selected?
    2. Setting up 6 cluster of 3 written and 3 reader in 3 VM setup.
    3. At code level have specific routing to master node in case of Lua script execution.
    4. Writing in Lua script a totally new language, proper execution and logging to make whole process atomic.
    5. Redis key issue when using Lua script {tenantId}, keys need to be passed  always to search from proper node in reeds.

  * **Marketing:**
    1. Setting up html template options with dynamic variables from customer uploaded file.
    2. Putting logic to insert tracking services details in email for open to download pixels, when user click on link routing.
    3. Putting limit on click and open to handle the AUTOMETIC api calls.
    4. Having unsubscription part with details.
    5. Analytics showing overall and campaigns specific stats till city and co-ordinates level using GeoLite2-City.mmdb

  * **Keycloak:**
    1. Properly testing multiple keycloak version which compatible with springbok with open license. Why choose keycloak?
    2. Undetanding the .ftl file and how exactly keycloak works.
    3. Learning client/Realm/Authetiaction settings
    4. Have to write custom code to connect kafka and login, registration pages
    5. Creating docker file with full dependencies fat jar
    6. Gmail configuration for OAuth2 login change and getting basic user details. Publishing in kafka topic.
    7. Creating service account for all the services for secure interaction.
    8. Working as a OAuth2 server for all login, validation.
    9. Setting up custom roles for user with JWT token with credentials
    10. Email notification  format  in .ftl file on creation of users.



  * **Tenant Service:**
    1. All registration of site, team, queue, agents, backend logic
    2. Email Oauth2 setup for outlook and Gmail App Passowed and OAUth2
    3. Workflow setup changes , making backend handle json and store in DN and redis in case of active.
    4. Handle ticket management with many api for assignment, status change, comments,
    5. For FAQs and Docs updating.
    6. Interacting with Keycloak for OAuth 2 when agent register


  * **Authentication & Security:**
    1. Writing custom Authorisation check on controller level with role and scope. Why choose custom and not spring?
    2. Check Pro Plan based filtering for marketing
    3. All secrets of gmail and outlook encrypted SHA-256 and stored in azure key vault




  * **Agent-dashboard:**
    1. Complete setup for OAuth2 token usage.
    2. Websocket connection& reconnection on page reloads.
    3. Chat life data transfer and listing to new messages.
    4. Workflow diagram react flow graph usage
    5. Roles based access in UI.
    6. Dynamic changes of menus with read/write access
    7. Analytics Graph interaction.
    8. Dynamic ticket dashboard.
    9. OAuth2 integration with keycloak
    10. Setup of Nginx for routing
    11. Customisable React jsx/Javascript Chat widget generation



  * **Agent Finder Service:**
    1. Why choose to publish in kafka topic all details for agent search, why not store all contact connection request in redis. Reason: If we store in rediss we have to write cron in agent finder to look for waiting , instead we choose kafka which will listen to all the request and try finding agent. If no agent find then cron will try finding the agent using cron and looking into redis.
    2. At business logic, having common search between contact and agent at language level, then rating at mathcing tags. With Max 3 concurrent chat.


  * **Local Execution :**
    1. Have to write all codes in dockerised way, proper configuration and scripting. Why choose docker?
    2. Have env specific configuration in docker.
    3. Pushing docker images to GitHub packages for deployment in prod.
    4. Creating Docker file for proper dependency using Cmd/EntryPoint
    5. Creating a TrackingId in complete flow, challenging as webflux doesnâ€™t follow threads.


  * **Agent-Service:**
    1. Cron to handle the agent logout if agent close browser directly, also have a frontend cron calling backend API with agent status.
    2. In cache storing all agent details when agent becomes available for chat. Like agent details with team, site, languages, tags,  connected contact sessions also the kafka partition details used for listening to the topic and sending to frontend using websocket.  Also while sending message publishing to contact listening partition specifically.
    3. If agent disconnect on frontend, cache put on TTL and if frontend reconnects with backend, updating of partition listener details.
    4. Listing to all the contact message storage and publishing to webscket for frontend.


  * **ContactService:**
    1. Frontend script to have chatbot web socket interaction with custom javascript.
    2. Stores all user select options in chatbot, publishes contact activity.
    3. Store contact info in cache, and when websocket disconnect give tel to redid. If contact reconnect kafka partition details updated .


---
* [x] **You worked as Technical Advisor - what due diligence process did you follow?**
  * I visited their Changsha facility for on-site technical assessment. My approach had three key phases:
    
    **1. Code Quality Assessment**
    I reviewed their repositoryâ€”primarily Java for backend services, Python for data pipelines, and native Android for mobile apps. I examined architecture patterns, API design, database schemas, and their IoT device communication protocols. I checked code modularity, error handling, test coverage, and looked for technical debt or hardcoded dependencies.
    
    **2. Documentation & Knowledge Transfer Analysis**
    Their documentation was inconsistentâ€”some modules well-documented, others had minimal comments, some in Mandarin. I flagged this as a major risk for knowledge transfer. I evaluated their technical specs, API documentation, and deployment procedures to understand the learning curve.
    
    **3. Team & Effort Estimation**
    Based on complexity and code size, I recommended a team of 8-10 engineers: 4 Java developers for backend, 2 Android developers, 1 Python engineer, 2 DevOps for their cloud infrastructure, plus QA resources. I estimated 6-8 months for complete ownership with initial 3 months requiring their team's overlap for knowledge transfer.
    
    **Coding Standards:**
    Mixed qualityâ€”good use of Spring framework and RESTful patterns, decent test coverage around 60%, but inconsistent naming conventions, insufficient exception handling, and lack of proper logging in critical modules. Their mobile code followed standard Android practices but needed refactoring for maintainability.
    
    **Outcome:**
    I delivered a detailed report with risk assessment, refactoring priorities, team structure, and cost projections that helped leadership make an informed acquisition decision.

###    Cross-Functional Experience
* [x] **You've worked across AdTech, Creative platforms, SaaS - how do you adapt to different domains?**
  * I focus on **fundamentals over domain specifics**. While AdTech, Creative platforms, and SaaS have different 
    business models, the underlying engineering principlesâ€”scalability, performance, data consistency, and user experienceâ€”remain constant.
    
    * **Domain Learning Strategy:**
    
    When entering a new domain, I spend the first 2-3 weeks understanding the business model and key metrics. In AdTech, I learned about RTB, CPM/CPC models, and impression tracking. For Creative platforms, I focused on asset management, rendering pipelines, and collaboration workflows. In SaaS, I prioritized multi-tenancy, subscription models, and usage analytics.
    
    I don't try to become a domain expert overnightâ€”I partner with product managers and business stakeholders who know the domain deeply, while I bring technical expertise.
    
    * **Technical Translation:**
    
    I identify domain-specific technical challenges. In AdTech, it was handling millions of bid requests per second with sub-100ms latency. For Creative platforms, it was optimizing large file handling and real-time collaboration. In SaaS, it was building robust tenant isolation and ensuring data security.
    
    These different challenges taught me various aspects of system designâ€”high-throughput systems, media processing, and secure multi-tenant architecture.
    
    * **Pattern Recognition:**
    
    After working across domains, I've developed pattern recognition skills. Authentication, caching strategies, API design, database optimizationâ€”these patterns repeat across domains with slight variations. This allows me to ramp up faster with each new domain.

    **The Key:** I stay curious, ask questions without ego, and focus on solving technical problems rather than getting caught up in domain jargon. The technology serves the businessâ€”understanding *why* something matters helps me build *what* matters.
---
* [x] **Describe your experience with on-site international code reviews (China)**
  * **Context:**

    I traveled to Changsha, China as Technical Advisor for a Bangalore IoT company evaluating a code base acquisition. This required navigating technical, cultural, and logistical challenges.
    
  * **Preparation:**
    
    Before the visit, I coordinated timezone-appropriate meetings, ensured NDA and IP agreements were in place, and prepared a structured review checklist. I also arranged for a translator, though their senior engineers spoke reasonable English.
    
  * **The Code Review Process:**
    
    I spent 4-5 days on-site doing hands-on reviews. I sat with their development team, walked through their Java backend services, Python data processing modules, and Android mobile code. I used their development environment to run the code, check build processes, and review their Git history to understand code evolution and commit patterns.
    
  * **Key Challenges:**
    
    * **Language barrier:** Code comments and some documentation were in Mandarin. Variable names were in English but 
    internal documentation required translation. I worked with their tech lead to clarify critical sections.
    
    * **Cultural differences:** Their team was initially hesitant to discuss technical debt or problemsâ€”they wanted to 
    present everything positively. I built rapport by being collaborative rather than interrogative, focusing on understanding their architecture decisions rather than criticizing.
    
    * **Time constraints:** I had limited days, so I prioritized critical modulesâ€”authentication, device communication 
    protocols, payment integration, and data security components.
    
  * **What I Evaluated:**
    
    Code structure, design patterns, test coverage, dependency management, scalability concerns, security practices, deployment infrastructure, and technical debt hotspots.
    
  * **Outcome:**
    
    I documented findings in real-time, had daily debriefs with their CTO, and delivered a comprehensive technical assessment that became the foundation for acquisition negotiations and transition planning.

## SCENARIO-BASED QUESTIONS
* [x] **Scenario 1: Your Kafka cluster is experiencing high consumer lag. How would you debug and resolve it?**
    * **1. Quick Diagnosis:**
    I'd first check where the bottleneck isâ€”run `kafka-consumer-groups --describe` to see lag per partition, check if it's specific partitions or all, and monitor consumer processing rate vs producer rate.

    **2. Common Issues I Check:**
    - **Consumer side:** Slow processing logic, insufficient consumer instances, or resource constraints (CPU/memory)
    - **Producer side:** Sudden traffic spike
    - **Broker side:** Disk I/O or network issues

    **3. Immediate Fixes:**
    - **Scale horizontally:** Add more consumer instances if we have more partitions than consumers
    - **Optimize processing:** Profile the consumer codeâ€”often it's blocking I/O or inefficient database calls. I'd move to async processing or batch operations
    - **Tune configs:** Adjust `max.poll.records` or `fetch.min.bytes` based on message size and processing speed

    **4. Long-term Solution:**
    If consistently hitting limits, increase partition count and optimize the processing logicâ€”remove blocking calls, add caching, use thread pools for parallel processing.

    **Real Example:**
    In AdTech, we had lag spikes during peak hours. Root cause was synchronous database writes blocking consumers. Fixed by batching writes and using async processingâ€”brought lag from 2M messages down to under 10K.

    The key is: identify bottleneck â†’ quick fix to stabilize â†’ optimize for long-term.
---
* [x] **Scenario 2: A microservice is experiencing intermittent 500 errors. How would you troubleshoot using your 
  monitoring stack?**
  
  * **My Systematic Approach:**

      **1. Immediate Checks - Correlation Analysis:**
      - Check monitoring dashboards for error rate spike timing
      - Correlate with deployment historyâ€”did we recently deploy?
      - Look at request volumeâ€”is it load-related?
      - Check which endpoints are failing using APM tools

      **2. Distributed Tracing:**
      I use tools like Jaeger or Zipkin to trace failed requests end-to-end. This shows me:
      - Which downstream service is timing out or erroring
      - Where latency spikes occur
      - If it's a specific dependency causing failures

      **3. Log Analysis:**
      - Aggregate logs using ELK/Splunk and filter for 500 errors
      - Look for exception stack traces, timeout errors, or connection pool exhaustion
      - Check for patternsâ€”specific user IDs, request types, or time windows

      **4. Resource Metrics:**
      Check Prometheus/Grafana for:
      - CPU/memory spikes that might cause timeouts
      - Database connection pool saturation
      - Thread pool exhaustion
      - Disk I/O or network bottlenecks

      **5. Common Root Causes I've Seen:**
      - **Database issues:** Connection timeouts, slow queries, deadlocks
      - **External API failures:** Third-party service degradation
      - **Resource exhaustion:** Thread pools, connection pools maxed out
      - **Race conditions:** Intermittent concurrency bugs
      - **Circuit breaker tripping:** Cascading failures from downstream services

      **6. Resolution:**
      Once identified, quick fixes might be:
      - Increase connection pool size if exhausted
      - Add circuit breakers or retries for flaky dependencies
      - Scale up if resource-constrained
      - Rollback if deployment-related

      **Example:** Once had intermittent 500s in payment service. Tracing showed Redis connection timeouts during traffic spikes. Fixed by increasing connection pool and adding connection retry logic.
---
* [x] **Scenario 3: Your Redis cluster is running out of memory. What steps would you take?**
  
    **My Approach:**

    **1. Immediate Assessment:**
    - Check `INFO memory` to see current usage and peak memory
    - Identify which keys are consuming most memory using `MEMORY USAGE` or `redis-rdb-tools`
    - Check eviction policyâ€”is it `noeviction` causing write failures?
    - Monitor hit/miss ratio to understand cache efficiency

    **2. Quick Wins:**
    - **Enable eviction:** Switch to `allkeys-lru` or `volatile-lru` if not set
    - **Set TTLs:** Find keys without expiration using `SCAN` and set appropriate TTLs
    - **Delete stale data:** Identify and remove obsolete keys
    - **Check for memory leaks:** Look for keys growing unexpectedly

    **3. Data Analysis:**
    I analyze key patterns:
    - Are we caching too much unnecessary data?
    - Large keys that should be broken down or moved elsewhere?
    - Duplicate or redundant data?

    **4. Real Example - Cisco Customer Engagement Pipeline:**

    We hit Redis memory limits in our customer engagement tracking system. Investigation showed:
    - **Root cause:** We were caching entire customer interaction histories (JSON objects) for real-time dashboard APIs
    - **Problem:** Some enterprise customers had 100K+ interaction records, creating multi-MB keys
    - **Memory pattern:** Top 5% of customers consumed 60% of Redis memory

    **Resolution:**
    - Changed caching strategyâ€”stored only last 30 days of interactions in Redis
    - Moved historical data queries to database with pagination
    - Implemented key compression for large JSON payloads
    - Added monitoring alerts at 80% memory threshold
    - Increased cluster size as interim solution

    **Long-term Fix:**
    - Moved aggregated metrics to Redis, raw data to database
    - Implemented TTL-based eviction for all cache keys
    - Added memory budgeting per data type

    **Key Takeaway:** In data pipelines, Redis memory issues often stem from caching too much raw data instead of computed aggregates.
---
* [x] **Scenario 4: You need to migrate from monolith to microservices. What's your approach?**
 
    **My Approach:**

    **1. Assessment & Planning:**
    - Analyze the monolithâ€”identify bounded contexts and service boundaries
    - Map dependencies and data flows
    - Prioritize services for extraction based on business value and technical feasibility
    - **Never do big-bang migration**â€”always incremental, strangler fig pattern

    **2. Identify First Candidate:**
    I look for modules that are:
    - Loosely coupled with clear boundaries
    - High business value or frequently changing
    - Performance bottlenecks that need independent scaling
    - Examples: notification service, payment processing, reporting

    **3. Extraction Strategy:**
    - Create API gateway/reverse proxy (Kong, Nginx)
    - Extract service behind the gateway while keeping monolith running
    - Use **Strangler Fig pattern**â€”route specific requests to new service, rest to monolith
    - Implement feature flags for gradual traffic shifting
    - Duplicate data temporarily if needed, sync using CDC or dual writes

    **4. Data Migration:**
    - Start with shared database, then separate schemas
    - Eventually move to separate databases per service
    - Use event-driven architecture (Kafka) for cross-service communication

    **5. Real Example - Cisco Customer Engagement Platform:**

    We migrated a monolithic customer engagement platform to microservices:

    **First extraction:** Real-time analytics service
    - **Why:** Heavy computation was slowing down the entire monolith
    - **Approach:** Extracted analytics processing, kept using same database initially
    - Routed `/analytics/*` API calls to new service via API gateway
    - Used Kafka for event streaming from monolith to analytics service

    **Second:** Notification service
    - Email/SMS notifications were independent and high-volume
    - Extracted with separate database for delivery tracking
    - Monolith published events to Kafka, notification service consumed

    **Challenges faced:**
    - Distributed transactionsâ€”moved to eventual consistency with saga pattern
    - Data consistency during dual-write phaseâ€”used Debezium for CDC
    - Increased latency for cross-service callsâ€”added caching and async processing

    **Key Principles:**
    - Migrate incrementally, one service at a time
    - Keep monolith runningâ€”de-risk with parallel execution
    - Invest in observability earlyâ€”distributed tracing, centralized logging
    - Don't over-fragmentâ€”start with 3-5 meaningful services, not 30
---
* [x] **Scenario 5: How would you design a system to handle 10 million email classifications per day?**
 
    **My Approach:**

    **1. Requirements Breakdown:**
    - 10M emails/day â‰ˆ 115 emails/second average, peak could be 300-500/sec
    - Latency requirements? Real-time vs batch processing?
    - Classification types: spam, category, sentiment, priority?
    - Accuracy vs speed tradeoffs

    **2. High-Level Architecture:**

    **Ingestion Layer:**
    - Kafka for message queueâ€”handles bursts, provides replay capability
    - Partitioned by tenant/domain for parallel processing
    - Producers push emails as events

    **Processing Layer:**
    - Consumer service pool (auto-scaling based on lag)
    - ML model servingâ€”pre-trained models loaded in memory or model serving platform like TensorFlow Serving
    - For 115/sec, I'd start with 10-15 consumer instances, each handling 10-15 emails/sec

    **Classification Strategy:**
    - **Lightweight pre-filtering:** Rule-based checks first (blacklist, whitelist)
    - **ML inference:** NLP models for content classification
    - **Batch optimization:** Process in micro-batches of 50-100 for GPU efficiency if using deep learning

    **Storage:**
    - Results to PostgreSQL/MongoDB for querying
    - Raw emails to S3/object storage for audit/retraining
    - Cache frequently accessed results in Redis

    **3. Scalability Considerations:**
    - Horizontal scaling of consumers based on Kafka lag
    - Model versioningâ€”A/B test new models on subset
    - Circuit breakers for downstream services
    - Rate limiting per tenant if multi-tenant

    **4. Real Example - AdTech Email Campaign Classification:**

    At my AdTech role, we classified marketing emails for campaign effectiveness:

    **Setup:**
    - Kafka with 20 partitions for parallel processing
    - 12 consumer instances running Python with scikit-learn models
    - Each consumer: fetch email â†’ extract features â†’ classify â†’ store results
    - Processing time: ~200ms per email including DB write

    **Optimizations we made:**
    - Cached feature extraction for similar emails (fingerprinting)
    - Batched database writesâ€”accumulated 100 results, bulk insert every 5 seconds
    - Used connection pooling to avoid DB bottleneck
    - Separated read replicas for analytics queries

    **Monitoring:**
    - Kafka consumer lag alerts
    - Classification accuracy metrics
    - Processing latency P95/P99
    - Error rate tracking

    **Cost optimization:**
    - Off-peak processing for non-urgent classifications
    - Spot instances for consumer workers
    - Model compression to reduce memory footprint

    **Key Metrics Achieved:**
    - Average latency: 180ms per email
    - Peak throughput: 400 emails/sec
    - 99.5% uptime

    The key is: Start simple, measure, optimize bottlenecks, scale horizontally.
---
* [x] **Scenario 6: Your Kubernetes pods are in CrashLoopBackOff. How do you debug?**

    **My Approach:**

    **1. Immediate Investigation:**
    ```bash
    kubectl get pods -n <namespace>  # Check pod status
    kubectl describe pod <pod-name>  # See events and error messages
    kubectl logs <pod-name>          # Current logs
    kubectl logs <pod-name> --previous  # Logs from crashed container
    ```

    **2. Common Root Causes I Check:**

    **Application Issues:**
    - Application crash on startupâ€”check logs for stack traces
    - Missing environment variables or config
    - Failed health checks (liveness/readiness probes too aggressive)

    **Resource Issues:**
    - OOMKilledâ€”memory limit too low, check `describe pod` for last state reason
    - CPU throttling preventing startup
    - Insufficient resources on nodes

    **Configuration Issues:**
    - ConfigMap/Secret not mounted or missing
    - Wrong image tag or image pull errors
    - Volume mount failuresâ€”PVC not bound

    **Dependency Issues:**
    - Can't connect to database/Redis on startup
    - External service unavailable during init

    **3. Debugging Steps:**

    ```bash
    # Check events
    kubectl describe pod <pod-name> | grep -A 10 Events

    # Check resource limits
    kubectl describe pod <pod-name> | grep -A 5 Limits

    # Get into running pod if possible
    kubectl exec -it <pod-name> -- /bin/sh

    # Check node resources
    kubectl top nodes
    kubectl describe node <node-name>
    ```

    **4. Real Example - Cisco Customer Engagement Pipeline:**

    We had CrashLoopBackOff on our data processing service after deployment:

    **Investigation:**
    - `kubectl logs` showed: "Connection refused to PostgreSQL"
    - Pod was trying to connect to DB immediately on startup
    - DB service was slow to initialize in new cluster

    **Root cause:** Application didn't have retry logic, failed if DB wasn't ready

    **Quick Fix:**
    - Added init container to wait for DB availability:
    ```yaml
    initContainers:
    - name: wait-for-db
      image: busybox
      command: ['sh', '-c', 'until nc -z postgres-service 5432; do sleep 2; done']
    ```

    **Permanent Fix:**
    - Added exponential backoff retry logic in application
    - Increased `initialDelaySeconds` on liveness probe from 10s to 30s
    - Set proper resource limits after profiling actual usage

    **5. Quick Troubleshooting Checklist:**
    - Image pull successful? Check `ImagePullBackOff`
    - Logs show error? Application crash
    - OOMKilled in describe? Increase memory limits
    - Probe failing? Adjust timing or fix endpoint
    - Environment vars missing? Check ConfigMap/Secret

    **Key Insight:** Most CrashLoopBackOff issues are either resource constraints, missing dependencies, or overly aggressive health checks. Logs and describe usually reveal the issue quickly.
---
* [x] **Scenario 7: A deployment caused downtime. How would you implement safeguards?**
    **My Approach:**

    **1. Pre-Deployment Safeguards:**

    **CI/CD Pipeline:**
    - Automated testsâ€”unit, integration, e2e tests must pass
    - Code quality gatesâ€”SonarQube for code coverage thresholds
    - Security scanningâ€”container image vulnerabilities
    - Smoke tests in staging environment identical to production

    **Staging Environment:**
    - Deploy to staging first, run automated regression tests
    - Load testing for performance regression
    - Canary testing with real-like traffic patterns

    **2. Deployment Strategy:**

    **Blue-Green Deployment:**
    - Maintain two identical environments
    - Deploy to inactive environment, test, then switch traffic
    - Quick rollback by switching back

    **Canary Releases (My Preferred):**
    - Deploy to 5-10% of pods first
    - Monitor error rates, latency, resource usage for 15-30 minutes
    - Gradually increase to 25% â†’ 50% â†’ 100%
    - Auto-rollback if error threshold breached

    **Rolling Updates:**
    - Update pods incrementally with proper `maxUnavailable` and `maxSurge` settings
    - Health checks prevent routing to unhealthy pods

    **3. Runtime Safeguards:**

    **Health Checks:**
    ```yaml
    livenessProbe:
      httpGet:
        path: /health
      initialDelaySeconds: 30
    readinessProbe:
      httpGet:
        path: /ready
      periodSeconds: 5
    ```

    **Circuit Breakers:**
    - Prevent cascading failures from new deployments
    - Auto-disable problematic features

    **Feature Flags:**
    - Deploy code dark, enable features gradually
    - Kill switch to disable features without redeployment

    **4. Monitoring & Alerts:**
    - Real-time dashboards: error rate, latency P95/P99, throughput
    - Automated alerts at error rate > 1% or latency spike > 50%
    - Comparison metrics: current vs previous deployment

    **5. Real Example - Cisco Customer Engagement Platform:**

    **The Incident:**
    Deployed analytics service update that caused 15 minutes downtime during peak hours.

    **What went wrong:**
    - Database migration script locked tables
    - No canary deploymentâ€”went straight to 100%
    - Health checks passed but queries were timing out
    - Rollback took 10 minutes

    **Safeguards Implemented:**

    **Immediate:**
    - Automated canary deploymentâ€”5% for 10 mins, monitor, then proceed
    - Added deployment runbook with rollback commands ready
    - Separate database migration from app deployment
    - Database migrations in off-peak hours with table lock monitoring

    **Long-term:**
    - Implemented feature flags for risky features
    - Added synthetic monitoringâ€”automated transactions post-deployment
    - Created deployment checklist: staging tested âœ“, migration tested âœ“, rollback plan âœ“
    - Set up Slack alerts for error rate spikes during deployments
    - Post-deployment monitoring dashboardâ€”auto-display for 1 hour

    **Rollback Strategy:**
    ```bash
    # Automated rollback script
    kubectl rollout undo deployment/analytics-service
    kubectl rollout status deployment/analytics-service
    # Verify health checks pass
    ```

    **6. Key Principles:**
    - **Never deploy Friday evening or before holidays**
    - Always have rollback plan tested
    - Monitor actively for first 30-60 minutes post-deployment
    - Gradual rollout > big bang
    - Automate everythingâ€”manual steps cause errors

    **Metrics Tracked:**
    - Deployment success rate (target: >99%)
    - Mean time to recovery (MTTR)
    - Percentage of deployments requiring rollback

    This approach reduced our deployment-related incidents from 2-3/month to 1-2/quarter.
---
* [x] **Scenario 8: How would you optimize a slow-performing API endpoint?**
    
    **My Approach:**

    **1. Identify Bottleneck:**
    Use APM tools to see where time is spentâ€”database, external calls, or application logic. Check distributed tracing and look at P95/P99 latencies.

    **2. Common Fixes:**
    - **Database:** Add indexes, fix N+1 queries, optimize slow queries with EXPLAIN
    - **Caching:** Add Redis for frequently accessed data
    - **Application:** Use async calls, batch operations, add pagination

    **3. Real Example - Cisco Customer Engagement API:**

    **Problem:** `/api/customer/engagement-summary` taking 4-6 seconds

    **Root Cause:** 
    - Complex JOIN across 5 tables aggregating 12 months of customer interactions
    - Missing indexes on date filters
    - N+1 query pattern
    - No caching

    **Fixes Applied:**
    - Added composite index on `(customer_id, interaction_date)` â†’ reduced to 800ms
    - Fixed N+1 with proper JOIN query
    - Added Redis caching with 15-min TTL â†’ reduced to 150ms (85% cache hit rate)
    - Created materialized view for aggregations, refreshed hourly â†’ final 50ms

    **Result:** 4-6s â†’ 50ms, database CPU from 80% â†’ 20%

    **Key Steps:** Profile â†’ fix biggest bottleneck (usually database) â†’ cache â†’ measure impact. Always test with 10% traffic first.
---
* [ ] Scenario 9: Design a disaster recovery plan for the Convonest platform
* [x] **Scenario 10: How would you handle database schema migration in production with zero downtime?**
   
    **My Approach: Expand/Contract Pattern**

    **Phase 1 - Expand:**
    Add new column/table, keep old one. New column nullable or with default.

    **Phase 2 - Dual Write:**
    Deploy code that writes to BOTH old and new. Backfill existing data in batches.

    **Phase 3 - Switch Reads:**
    Deploy code reading from new column.

    **Phase 4 - Contract:**
    After weeks, drop old column when confident.

    **Real Example - Cisco:**

    Changed `customer_score` from INTEGER to DECIMAL on 10M+ row table.

    **Steps:**
    1. Added `customer_score_new DECIMAL(10,2)` column
    2. Updated app to write both columns
    3. Backfilled data in 10K row batches during off-peak
    4. Switched app to read new column
    5. After 4 weeks, dropped old column

    **Key:** Never drop old schema same time as adding new. Always overlap for safety.

    **Tools:** pt-online-schema-change for MySQL, Flyway for versioning.
---
* [x] **Scenario 11: A microservice is getting 401 Unauthorized when calling another service with Keycloak. How would 
  you debug?**
    # Debugging 401 Unauthorized with Keycloak

    **My Approach:**

    **1. Quick Checks:**
    - Is token present in request header? Check `Authorization: Bearer <token>`
    - Token expired? Check `exp` claim by decoding JWT at jwt.io
    - Wrong service endpoint or realm?
    - Network issue preventing service from reaching Keycloak?

    **2. Token Validation:**
    ```bash
    # Decode JWT to inspect claims
    echo $TOKEN | cut -d'.' -f2 | base64 -d | jq

    # Check: exp (expiration), iss (issuer), aud (audience), roles/scopes
    ```

    Common issues:
    - Token expired (check `exp` timestamp)
    - Wrong `aud` (audience) - token not meant for target service
    - Missing required roles/scopes
    - Wrong issuer URL

    **3. Keycloak-Specific Checks:**
    - Client ID/secret correct?
    - Service account has proper roles assigned?
    - Token endpoint responding? Test with curl:
    ```bash
    curl -X POST "https://keycloak/realms/myrealm/protocol/openid-connect/token" \
      -d "client_id=my-service" \
      -d "client_secret=secret" \
      -d "grant_type=client_credentials"
    ```

    **4. Service-Side Validation:**
    - Is the service configured with correct Keycloak realm URL?
    - Public key sync issue? Service might have stale Keycloak public keys
    - Check service logs for specific validation errors

    **5. Real Example - Cisco Customer Engagement:**

    **Problem:** Analytics service getting 401 when calling customer-data service.

    **Debug Steps:**
    - Checked logs: "Invalid token signature"
    - Decoded token: `iss` was `https://keycloak.old.com` but service expected `https://keycloak.new.com`
    - **Root cause:** We migrated Keycloak to new domain, analytics service still requesting tokens from old realm

    **Fix:** 
    - Updated analytics service Keycloak configuration to new realm URL
    - Restarted service to clear token cache

    **Other Common Issues I've Seen:**
    - Clock skew between services causing `exp` validation failure
    - Service restarted, lost cached Keycloak public keys
    - Token refresh logic broken, using expired tokens

    **Quick Debug Commands:**
    ```bash
    # Test token manually
    curl -H "Authorization: Bearer $TOKEN" https://service/api/endpoint

    # Check Keycloak logs
    kubectl logs keycloak-pod | grep "token"

    # Verify service can reach Keycloak
    curl https://keycloak/realms/myrealm/.well-known/openid-configuration
    ```

    **Key:** Decode the JWT first, check expiration and claims. Usually it's expired token, wrong audience, or misconfigured realm URL.
---
* [x] **Scenario 12: Service A needs to call Service B - walk through the complete authentication flow with Keycloak 
  service account tokens**
    
    **Quick Checks:**
    1. Token expired? Decode JWT at jwt.io, check `exp` claim
    2. Token present in `Authorization: Bearer` header?
    3. Check token claims: `aud` (audience), `iss` (issuer), roles/scopes

    **Common Issues:**
    - Expired token
    - Wrong audience - token not for target service
    - Missing roles/scopes
    - Wrong Keycloak realm URL
    - Service has stale public keys

    **Real Example - Cisco:**

    Analytics service â†’ 401 calling customer-data service.

    **Debug:**
    - Decoded token: `iss` was old Keycloak URL
    - Service expected new URL after migration
    - Fixed by updating realm config and restarting

    **Quick Fix:**
    ```bash
    # Decode token
    echo $TOKEN | cut -d'.' -f2 | base64 -d | jq
    # Check exp, aud, iss
    ```

    Usually: expired token or realm URL mismatch.

---
* [x] **Scenario 13: Your JWT tokens are expiring too frequently causing issues. What would you do?**
    
    **My Approach:**

    **1. Immediate Analysis:**
    - What's current TTL? Check `exp` claim in token
    - How often are users hitting 401s?
    - Are refresh tokens implemented?

    **2. Solutions:**

    **Short-term:**
    - Increase access token expiration (e.g., 5min â†’ 15min)
    - Balance security vs UX

    **Better Solution - Refresh Tokens:**
    - Keep access token short (5-15 min) for security
    - Issue refresh token (longer TTL: 7-30 days)
    - Client refreshes access token silently before expiry
    - User stays logged in without interruption

    **Implementation:**
    ```javascript
    // Check token expiry before API call
    if (tokenExpiresIn < 60s) {
      await refreshAccessToken();
    }
    makeApiCall();
    ```

    **3. Real Example - Cisco Customer Engagement:**

    **Problem:** Users getting logged out every 5 minutes during long analytics sessions.

    **Root Cause:** 
    - Access tokens set to 5min
    - No refresh token implementation
    - Users forced to re-login frequently

    **Fix:**
    - Implemented refresh token flow with 7-day expiry
    - Access token stays 5min (security)
    - Frontend auto-refreshes token 1min before expiry
    - Added token refresh on 401 response as fallback

    **Configuration in Keycloak:**
    - Access Token Lifespan: 5-15 minutes
    - Refresh Token Lifespan: 7-30 days
    - Enabled "Refresh Token" grant type

    **Result:** Zero user complaints, maintained security.

    **Key:** Short access tokens + long refresh tokens = security + good UX.
---
* [x] **Scenario 14: How would you handle a security incident where a service account token is compromised?**

    **Immediate Actions:**

    **1. Contain the Breach (First 5 minutes):**
    - Revoke the compromised token immediately in Keycloak
    - Disable the service account
    - Block suspicious IP addresses at firewall/API gateway
    - Check access logs for unauthorized activity

    **2. Assess Impact (First 30 minutes):**
    - Review audit logs - what resources were accessed?
    - Check timestamp of compromise - how long was it active?
    - Identify data exposure or modifications made
    - Check if other tokens/credentials accessed

    **3. Rotate & Secure (First hour):**
    - Generate new service account credentials
    - Rotate all related secrets (database passwords, API keys if accessed)
    - Update services with new credentials
    - Deploy changes

    **4. Monitor & Investigate:**
    - Set up alerts for the old token if somehow still used
    - Full security audit - how was it compromised?
    - Check for backdoors or persistent access

    **5. Real Example - Cisco:**

    **Incident:** Analytics service account token found in public GitHub repo.

    **Timeline:**

    **T+5min:** 
    - Revoked token in Keycloak
    - Disabled service account
    - Checked logs - no suspicious activity yet (caught early)

    **T+30min:**
    - Rotated service account credentials
    - Created new client secret
    - Deployed to all analytics service instances via CI/CD

    **T+1hr:**
    - Force-revoked all active sessions for that client
    - Added GitHub secret scanning alerts
    - Implemented pre-commit hooks to prevent credential commits

    **Prevention Measures Added:**
    - Vault for secret management - no hardcoded tokens
    - Short-lived tokens with auto-rotation
    - IP whitelisting for service accounts
    - Enhanced monitoring - alert on unusual service account behavior

    **6. Post-Incident:**
    - Document incident in security log
    - Team debrief - how to prevent recurrence
    - Update runbooks

    **Key Principle:** Revoke first, ask questions later. Speed matters in security incidents.

Interview Preparation Tips:
* âœ“ Be ready to explain "why" you made specific technology choices
* âœ“ Prepare real examples from your projects (Convonest, Zee5, Cisco, Picsart, Reliance)
* âœ“ Be ready to draw architecture diagrams
* âœ“ Know the numbers from your systems (throughput, latency, scale)
* âœ“ Be honest about what you know and don't know
* âœ“ For questions you haven't directly worked on, explain your approach to learn/solve it
* âœ“ Practice explaining your custom JWT decoder implementation
* âœ“ Be ready to code on whiteboard or shared editor
* âœ“ Review your Keycloak setup and service-to-service authentication flow
